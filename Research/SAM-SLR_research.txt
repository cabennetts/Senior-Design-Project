The first thing I found in my research was that the accuracy of the videos without depth information might not be as bad as I initially understood it to be. When I first found this project on PapersWithCode. The metric that was used for the WLASL dataset was different than the metric used for AUTSL, the turkish dataset, which let me to believe the performance was substantally different. However, once I looked into the research paper for the model, it appears the accuracies are very comperable. The finetuned accuracy with depth data was 98.52% and without the depth data it was 98.43% which for our purposes is basically the same. This might suggest we will be able to use the WLASL dataset after all since the extra information does not seem that impactful. The rest of my reseach was into trying to run test of the model by using the provided docker image. However, I do not have much experience with using docker images and it seems that docker is not a huge fan of me since I have yet to successfully run the provided docker image. Given the difficulties I have run into trying to actually run this code, I believe large modifications would be needed to suit this code to our needs. I think it is likely we will come to a middle ground of taking heavy inspiration from their code base, but creating a custom model that is suited to our purposes, but further research is still needed to fully come to a solid determination. 
