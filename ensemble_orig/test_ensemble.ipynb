{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble for one video\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bencl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# I hate python sooooooo much for making me do this\n",
    "sys.path.append(os.path.abspath('DataPreparation'))\n",
    "sys.path.append(os.path.abspath('models'))\n",
    "sys.path.append(os.path.abspath('FeatureExtraction'))\n",
    "sys.path.append(os.path.abspath('SL-GCN/data_gen/'))\n",
    "#sys.path.append(os.path.abspath('../SL-GCN/data_gen'))\n",
    "\n",
    "from Conv3D import r2plus1d_18\n",
    "import decouple_gcn_attn\n",
    "from T_Pose_model import T_Pose_model\n",
    "import split_video\n",
    "import demo\n",
    "import gen_frames\n",
    "import wholepose_features_extraction\n",
    "import sign_gendata\n",
    "import gen_bone_data\n",
    "import gen_motion_data\n",
    "import sign_27\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/rgb_final/rgb_final/sign_resnet2d+1_5_epoch009.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9236/2438245024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbone_motion_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/bone_motion_models/sign_bone_motion_final-25.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#decent sign_resnet2d+1_5_epoch009.pth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/rgb_final/rgb_final/sign_resnet2d+1_5_epoch009.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtcn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/TCN_models/T_Pose_model_16_99.0.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/rgb_final/rgb_final/sign_resnet2d+1_5_epoch009.pth'"
     ]
    }
   ],
   "source": [
    "joint_model = torch.load('D:/joint_models/sign_joint_final-24-95.pt')\n",
    "joint_motion_model = torch.load('D:/joint_motion_models/sign_joint_motion_final-32-86.pt')\n",
    "bone_model = torch.load('D:/bone_models/sign_bone_final-25.pt')\n",
    "bone_motion_model = torch.load('D:/bone_motion_models/sign_bone_motion_final-25.pt')\n",
    "#decent sign_resnet2d+1_5_epoch009.pth\n",
    "rgb_model = torch.load('D:/rgb_final/rgb_final_3layers_16frames_epoch9isbest/sign_resnet2d+1_5_epoch009.pth')\n",
    "tcn_model = torch.load('D:/TCN_models/T_Pose_model_16_99.0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is extracting the needed info from the video (bones, joints, frames, etc.)\n",
    "# It will save this info in the same directory as the video\n",
    "path_to_video = 'D:/output/i-have-school-tomorrow/'\n",
    "output_path = 'D:/output/hello-what-you-name/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into 16 frame sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video.run(path_to_video + \"i-have-school-tomorrow.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate npy files for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract .npy file from video using demo.py\n",
    "\n",
    "print(\"generating npy files\")\n",
    "#loop through each folder in the path_to_videos, and run demo.py on each video\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/npy/')\n",
    "        demo.run(path_to_video + folder, path_to_video + folder + '/npy/')\n",
    "print(\"done generating npy files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will use the .npy file to extract the frames of the video\n",
    "\n",
    "print(\"generating frames\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        #print(path_to_videos + folder + \"/16.mp4\")\n",
    "        os.mkdir(path_to_video + folder + '/frames/')\n",
    "        gen_frames.run(path_to_video + folder, path_to_video + folder + '/frames/', path_to_video + folder + '/npy/')\n",
    "print(\"done generating frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pt files for TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now extract .pt file from video using wholepose_features_extraction.py\n",
    "\n",
    "print(\"generating wholepose feature files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/pt/')\n",
    "        wholepose_features_extraction.run(path_to_video + folder, path_to_video + folder + '/pt/', False)\n",
    "print(\"done generating wholepose feature files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sign data for the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating sign data files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/sign_gen/')\n",
    "        sign_gendata.run(path_to_video + folder + '/npy/', path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating sign data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bone data for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating bone data files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        # bone data is saved into sign_gen folder\n",
    "        gen_bone_data.run(path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating bone data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate motion data for the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating motion data files for joint and bones\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        #os.mkdir(path_to_video + folder + '/sign_gen/')\n",
    "        gen_motion_data.run(path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating motion data files for joint and bones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for set_0:\n",
      "31.39% : tomorrow\n",
      "15.16% : yesterday\n",
      "13.30% : my\n",
      "12.50% : you\n",
      "12.44% : hello\n",
      "\n",
      "\n",
      "26.94% : hello\n",
      "19.71% : tomorrow\n",
      "15.11% : what\n",
      "12.01% : we\n",
      "7.07% : yesterday\n",
      "\n",
      "\n",
      "50.71% : hello\n",
      "14.93% : tomorrow\n",
      "8.05% : yesterday\n",
      "7.12% : go\n",
      "5.15% : my\n",
      "\n",
      "\n",
      "Predictions for set_1:\n",
      "36.11% : tomorrow\n",
      "19.61% : you\n",
      "14.33% : my\n",
      "12.42% : yesterday\n",
      "4.44% : have\n",
      "\n",
      "\n",
      "19.24% : car\n",
      "18.67% : have\n",
      "15.88% : you\n",
      "15.09% : yesterday\n",
      "14.53% : go\n",
      "\n",
      "\n",
      "40.12% : what\n",
      "26.03% : go\n",
      "11.56% : hello\n",
      "4.48% : school\n",
      "3.99% : car\n",
      "\n",
      "\n",
      "Predictions for set_2:\n",
      "30.15% : have\n",
      "14.93% : car\n",
      "10.98% : you\n",
      "10.46% : tomorrow\n",
      "9.90% : name\n",
      "\n",
      "\n",
      "46.68% : you\n",
      "21.45% : car\n",
      "10.02% : yesterday\n",
      "8.34% : tomorrow\n",
      "5.47% : have\n",
      "\n",
      "\n",
      "36.51% : you\n",
      "22.48% : tomorrow\n",
      "15.56% : we\n",
      "7.65% : my\n",
      "7.56% : self\n",
      "\n",
      "\n",
      "Predictions for set_3:\n",
      "20.49% : my\n",
      "18.94% : we\n",
      "17.58% : tomorrow\n",
      "13.46% : you\n",
      "8.15% : have\n",
      "\n",
      "\n",
      "19.14% : car\n",
      "18.24% : go\n",
      "17.41% : name\n",
      "12.37% : what\n",
      "11.88% : school\n",
      "\n",
      "\n",
      "48.87% : name\n",
      "26.37% : school\n",
      "5.30% : have\n",
      "4.66% : self\n",
      "3.14% : we\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize([240, 240]),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "#input_clips = []\n",
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "model = r2plus1d_18(pretrained=True, num_classes=13)\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in rgb_model.items():\n",
    "    #name = k[7:] # remove 'module.'\n",
    "    name = k.replace('module.', '')\n",
    "    new_state_dict[name]=v\n",
    "\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/frames/0'):\n",
    "        images = []\n",
    "        input_clips = []\n",
    "        for i, file in enumerate(os.listdir(path_to_video + folder + '/frames/0')):\n",
    "            if i < 4:\n",
    "                continue\n",
    "            image = Image.open(path_to_video + folder + '/frames/0/' + file)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            if len(images) == 16:\n",
    "                images = torch.stack(images, dim=0)\n",
    "                images = images.permute(1, 0, 2, 3)\n",
    "                images = torch.Tensor(images)\n",
    "                images = images.unsqueeze(0)\n",
    "                input_clips.append(images)\n",
    "                images = []\n",
    "\n",
    "#outputs_clips =[]\n",
    "#for i_clip in range(inputs_clips.size(1)):\n",
    "#    inputs = inputs_clips[:,i_clip,:,:]\n",
    "#    outputs_clips.append(model(inputs))\n",
    "#input = inputs_clips[:,i_clip,:,:]\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        for set in input_clips:\n",
    "            output = model(set)\n",
    "            # Convert the predictions to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "            # Get the top k probabilities and their indices\n",
    "            top_probs, top_idxs = probs.topk(5, dim=1)\n",
    "\n",
    "            # Convert indices to class labels\n",
    "            top_classes = [class_labels[idx] for idx in top_idxs[0]]\n",
    "\n",
    "            # Print the top k probabilities and their corresponding class labels\n",
    "            for i in range(5):\n",
    "                print(\"{:.2f}% : {}\".format(top_probs[0][i]*100, top_classes[i]))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Predictions (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "\n",
    "# remove unnecessary module. from state_dict\n",
    "joint_state_dict = OrderedDict()\n",
    "bone_state_dict = OrderedDict()\n",
    "joint_motion_state_dict = OrderedDict()\n",
    "bone_motion_state_dict = OrderedDict()\n",
    "for i, model in enumerate([joint_model, bone_model, joint_motion_model, bone_motion_model]):\n",
    "    for k, v in model.items():\n",
    "        #name = k[7:] # remove 'module.'\n",
    "        name = k.replace('module.', '')\n",
    "        if i == 0:\n",
    "            joint_state_dict[name]=v\n",
    "        elif i == 1:\n",
    "            bone_state_dict[name]=v\n",
    "        elif i == 2:\n",
    "            joint_motion_state_dict[name]=v\n",
    "        elif i == 3:\n",
    "            bone_motion_state_dict[name]=v\n",
    "\n",
    "# Load model architechure\n",
    "Model_j = decouple_gcn_attn.Model(13, 27, 1, 16, 41,  \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_b = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_jm = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_bm = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "\n",
    "# Load model states from checkpoints\n",
    "Model_j.load_state_dict(joint_state_dict)\n",
    "Model_b.load_state_dict(bone_state_dict)\n",
    "Model_jm.load_state_dict(joint_motion_state_dict)\n",
    "Model_bm.load_state_dict(bone_motion_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "Model_j.eval()\n",
    "Model_b.eval()\n",
    "Model_jm.eval()\n",
    "Model_bm.eval()\n",
    "\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/sign_gen/'):\n",
    "        bone_npy = np.load(path_to_video + folder + '/sign_gen/test_data_bone.npy')\n",
    "        joint_npy = np.load(path_to_video + folder + '/sign_gen/test_data_joint.npy')\n",
    "        bone_motion_npy = np.load(path_to_video + folder + '/sign_gen/test_data_bone_motion.npy')\n",
    "        joint_motion_npy = np.load(path_to_video + folder + '/sign_gen/test_data_joint_motion.npy')\n",
    "\n",
    "        # Load the data onto the GPU if available\n",
    "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        bone_npy = torch.from_numpy(bone_npy).to(device)\n",
    "        joint_npy = torch.from_numpy(joint_npy).to(device)\n",
    "        bone_motion_npy = torch.from_numpy(bone_motion_npy).to(device)\n",
    "        joint_motion_npy = torch.from_numpy(joint_motion_npy).to(device)\n",
    "\n",
    "        # Make predictions using the four models\n",
    "        with torch.no_grad():\n",
    "            joint_output = Model_j(joint_npy)\n",
    "            bone_output = Model_b(bone_npy)\n",
    "            joint_motion_output = Model_jm(joint_motion_npy)\n",
    "            bone_motion_output = Model_bm(bone_motion_npy)\n",
    "\n",
    "        # Print the top 5 predictions and their confidence percentages for each model\n",
    "        def print_top_5(output):\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "            for i in range(5):\n",
    "                print(f\"Prediction {i+1}: {class_labels[top_5_indices[0][i]]}: {top_5_probs[0][i]*100:.2f}%\")\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        print(\"Joint Model:\")\n",
    "        print_top_5(joint_output)\n",
    "        print(\"Bone Model:\")\n",
    "        print_top_5(bone_output)\n",
    "        print(\"Joint Motion Model:\")\n",
    "        print_top_5(joint_motion_output)\n",
    "        print(\"Bone Motion Model:\")\n",
    "        print_top_5(bone_motion_output)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Predictions (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = T_Pose_model(frames_number=60,joints_number=33,\n",
    "    n_classes=13\n",
    ")\n",
    "#model = nn.DataParallel(model)    \n",
    "model = model.to(device)\n",
    "tcn_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in tcn_model.items():\n",
    "    #name = k[7:] # remove 'module.'\n",
    "    name = k.replace('module.', '')\n",
    "    tcn_state_dict[name]=v\n",
    "\n",
    "# Add weights from checkpoint model\n",
    "model.load_state_dict(tcn_state_dict)#,strict=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/pt/'):\n",
    "        pt_file = path_to_video + folder + '/pt/0.mp4.pt'\n",
    "        data = torch.load(pt_file)\n",
    "        #data = data.contiguous().view(1,-1,24,24)\n",
    "        data_in = torch.autograd.Variable(data.to(device), requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            pred=model(data_in)\n",
    "        #pred = pred.cpu().detach().numpy()\n",
    "        def print_top_5(output):\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "            for i in range(5):\n",
    "                print(f\"Prediction {i+1}: {class_labels[top_5_indices[0][i]]}: {top_5_probs[0][i]*100:.2f}%\")\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        print_top_5(pred)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ensemble\n",
    "\n",
    "#run ensemble's main function\n",
    "ensemble.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f37eb7a0b74f2e22d6fd383ec5ebb97eab96881d30ca476017b85fc7b23292a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
