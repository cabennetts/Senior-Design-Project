{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble for one video\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# I hate python sooooooo much for making me do this\n",
    "sys.path.append(os.path.abspath('DataPreparation'))\n",
    "sys.path.append(os.path.abspath('models'))\n",
    "sys.path.append(os.path.abspath('FeatureExtraction'))\n",
    "sys.path.append(os.path.abspath('SL-GCN/data_gen/'))\n",
    "#sys.path.append(os.path.abspath('../SL-GCN/data_gen'))\n",
    "\n",
    "from Conv3D import r2plus1d_18\n",
    "import decouple_gcn_attn\n",
    "from T_Pose_model import T_Pose_model\n",
    "import split_video\n",
    "import demo\n",
    "import gen_frames\n",
    "import wholepose_features_extraction\n",
    "import sign_gendata\n",
    "import gen_bone_data\n",
    "import gen_motion_data\n",
    "import sign_27\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = torch.load('D:/joint_models/sign_joint_final-24-95.pt')\n",
    "joint_motion_model = torch.load('D:/joint_motion_models/sign_joint_motion_final-32-86.pt')\n",
    "bone_model = torch.load('D:/bone_models/sign_bone_final-25.pt')\n",
    "bone_motion_model = torch.load('D:/bone_motion_models/sign_bone_motion_final-25.pt')\n",
    "#decent sign_resnet2d+1_5_epoch009.pth\n",
    "rgb_model = torch.load('D:/rgb_final/rgb_final_3layers_16frames_epoch9isbest/sign_resnet2d+1_5_epoch009.pth')\n",
    "tcn_model = torch.load('D:/TCN_models/T_Pose_model_16_99.0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is extracting the needed info from the video (bones, joints, frames, etc.)\n",
    "# It will save this info in the same directory as the video\n",
    "path_to_video = 'D:/output/i-have-school-tomorrow/'\n",
    "#output_path = 'D:/output/hello-what-you-name/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into 16 frame sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video.run(path_to_video + \"i-have-school-tomorrow.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate npy files for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract .npy file from video using demo.py\n",
    "\n",
    "print(\"generating npy files\")\n",
    "#loop through each folder in the path_to_videos, and run demo.py on each video\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/npy/')\n",
    "        demo.run(path_to_video + folder, path_to_video + folder + '/npy/')\n",
    "print(\"done generating npy files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will use the .npy file to extract the frames of the video\n",
    "\n",
    "print(\"generating frames\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        #print(path_to_videos + folder + \"/16.mp4\")\n",
    "        os.mkdir(path_to_video + folder + '/frames/')\n",
    "        gen_frames.run(path_to_video + folder, path_to_video + folder + '/frames/', path_to_video + folder + '/npy/')\n",
    "print(\"done generating frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pt files for TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now extract .pt file from video using wholepose_features_extraction.py\n",
    "\n",
    "print(\"generating wholepose feature files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/pt/')\n",
    "        wholepose_features_extraction.run(path_to_video + folder, path_to_video + folder + '/pt/', False)\n",
    "print(\"done generating wholepose feature files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sign data for the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating sign data files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        os.mkdir(path_to_video + folder + '/sign_gen/')\n",
    "        sign_gendata.run(path_to_video + folder + '/npy/', path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating sign data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bone data for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating bone data files\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        # bone data is saved into sign_gen folder\n",
    "        gen_bone_data.run(path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating bone data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate motion data for the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating motion data files for joint and bones\")\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder):\n",
    "        #os.mkdir(path_to_video + folder + '/sign_gen/')\n",
    "        gen_motion_data.run(path_to_video + folder + '/sign_gen/')\n",
    "print(\"done generating motion data files for joint and bones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bencl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bencl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for set_0:\n",
      "Predictions for set_1:\n",
      "Predictions for set_2:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 132710400 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7140/68194329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_clips\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mcurrent_set_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;31m# Convert the predictions to probabilities using softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ensemble\\models\\Conv3D.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr2plus1d_18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m# print(out.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Flatten the layer to fc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\video\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             )\n\u001b[1;32m--> 608\u001b[1;33m         return F.conv3d(\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 132710400 bytes."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize([240, 240]),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "#input_clips = []\n",
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "model = r2plus1d_18(pretrained=True, num_classes=13)\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in rgb_model.items():\n",
    "    #name = k[7:] # remove 'module.'\n",
    "    name = k.replace('module.', '')\n",
    "    new_state_dict[name]=v\n",
    "\n",
    "# Will be in form of a list of lists of tuples (percent, label)\n",
    "cnn_predictions = []\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/frames/0'):\n",
    "        images = []\n",
    "        input_clips = []\n",
    "        for i, file in enumerate(os.listdir(path_to_video + folder + '/frames/0')):\n",
    "            if i < 4:\n",
    "                continue\n",
    "            image = Image.open(path_to_video + folder + '/frames/0/' + file)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            if len(images) == 16:\n",
    "                images = torch.stack(images, dim=0)\n",
    "                images = images.permute(1, 0, 2, 3)\n",
    "                images = torch.Tensor(images)\n",
    "                images = images.unsqueeze(0)\n",
    "                input_clips.append(images)\n",
    "                images = []\n",
    "\n",
    "#outputs_clips =[]\n",
    "#for i_clip in range(inputs_clips.size(1)):\n",
    "#    inputs = inputs_clips[:,i_clip,:,:]\n",
    "#    outputs_clips.append(model(inputs))\n",
    "#input = inputs_clips[:,i_clip,:,:]\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        for set in input_clips:\n",
    "            current_set_preds = []\n",
    "            output = model(set)\n",
    "            # Convert the predictions to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "            # Get the top k probabilities and their indices\n",
    "            top_probs, top_idxs = probs.topk(5, dim=1)\n",
    "\n",
    "            # Convert indices to class labels\n",
    "            top_classes = [class_labels[idx] for idx in top_idxs[0]]\n",
    "\n",
    "            # Print the top k probabilities and their corresponding class labels\n",
    "            for i in range(3):\n",
    "                current_set_preds.append((top_probs[0][i]*100, top_classes[i]))\n",
    "            #print(\"\\n\")\n",
    "            cnn_predictions.append(current_set_preds)\n",
    "print(cnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Predictions (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ensemble\\models\\decouple_gcn_attn.py:29: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(conv.weight, mode='fan_out')\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:30: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(conv.bias, 0)\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:34: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(bn.weight, scale)\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:35: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(bn.bias, 0)\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:113: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.Linear_bias, 1e-6)\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.eyes = nn.Parameter(torch.tensor(torch.stack(\n",
      "d:\\ensemble\\models\\decouple_gcn_attn.py:255: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.fc.weight, 0, math.sqrt(2. / num_class))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for set_0:\n",
      "Joint Model:\n",
      "Prediction 1: you: 54.35%\n",
      "Prediction 2: what: 31.85%\n",
      "Prediction 3: hello: 6.96%\n",
      "Prediction 4: self: 4.23%\n",
      "Prediction 5: tomorrow: 1.75%\n",
      "Bone Model:\n",
      "Prediction 1: self: 85.82%\n",
      "Prediction 2: my: 6.94%\n",
      "Prediction 3: tomorrow: 2.55%\n",
      "Prediction 4: yesterday: 1.91%\n",
      "Prediction 5: you: 1.84%\n",
      "Joint Motion Model:\n",
      "Prediction 1: tomorrow: 98.41%\n",
      "Prediction 2: self: 0.82%\n",
      "Prediction 3: we: 0.56%\n",
      "Prediction 4: you: 0.21%\n",
      "Prediction 5: my: 0.01%\n",
      "Bone Motion Model:\n",
      "Prediction 1: self: 69.44%\n",
      "Prediction 2: my: 30.31%\n",
      "Prediction 3: we: 0.16%\n",
      "Prediction 4: you: 0.05%\n",
      "Prediction 5: hello: 0.02%\n",
      "\n",
      "\n",
      "Predictions for set_1:\n",
      "Joint Model:\n",
      "Prediction 1: what: 95.88%\n",
      "Prediction 2: you: 2.90%\n",
      "Prediction 3: hello: 0.52%\n",
      "Prediction 4: self: 0.36%\n",
      "Prediction 5: name: 0.11%\n",
      "Bone Model:\n",
      "Prediction 1: tomorrow: 28.97%\n",
      "Prediction 2: have: 22.49%\n",
      "Prediction 3: yesterday: 15.63%\n",
      "Prediction 4: self: 12.98%\n",
      "Prediction 5: what: 7.96%\n",
      "Joint Motion Model:\n",
      "Prediction 1: you: 59.64%\n",
      "Prediction 2: self: 19.03%\n",
      "Prediction 3: we: 11.86%\n",
      "Prediction 4: tomorrow: 7.26%\n",
      "Prediction 5: hello: 0.72%\n",
      "Bone Motion Model:\n",
      "Prediction 1: self: 53.00%\n",
      "Prediction 2: name: 41.02%\n",
      "Prediction 3: we: 2.06%\n",
      "Prediction 4: yesterday: 1.36%\n",
      "Prediction 5: you: 0.82%\n",
      "\n",
      "\n",
      "Predictions for set_2:\n",
      "Joint Model:\n",
      "Prediction 1: what: 99.21%\n",
      "Prediction 2: name: 0.41%\n",
      "Prediction 3: you: 0.19%\n",
      "Prediction 4: car: 0.10%\n",
      "Prediction 5: self: 0.07%\n",
      "Bone Model:\n",
      "Prediction 1: have: 56.68%\n",
      "Prediction 2: go: 42.98%\n",
      "Prediction 3: what: 0.11%\n",
      "Prediction 4: school: 0.08%\n",
      "Prediction 5: tomorrow: 0.04%\n",
      "Joint Motion Model:\n",
      "Prediction 1: go: 54.58%\n",
      "Prediction 2: have: 40.55%\n",
      "Prediction 3: what: 3.76%\n",
      "Prediction 4: you: 0.80%\n",
      "Prediction 5: school: 0.19%\n",
      "Bone Motion Model:\n",
      "Prediction 1: school: 81.50%\n",
      "Prediction 2: name: 14.89%\n",
      "Prediction 3: what: 2.85%\n",
      "Prediction 4: go: 0.31%\n",
      "Prediction 5: have: 0.23%\n",
      "\n",
      "\n",
      "Predictions for set_3:\n",
      "Joint Model:\n",
      "Prediction 1: what: 89.41%\n",
      "Prediction 2: you: 7.87%\n",
      "Prediction 3: hello: 1.19%\n",
      "Prediction 4: self: 0.81%\n",
      "Prediction 5: name: 0.30%\n",
      "Bone Model:\n",
      "Prediction 1: school: 59.91%\n",
      "Prediction 2: tomorrow: 31.74%\n",
      "Prediction 3: car: 2.37%\n",
      "Prediction 4: name: 2.32%\n",
      "Prediction 5: yesterday: 2.25%\n",
      "Joint Motion Model:\n",
      "Prediction 1: name: 51.21%\n",
      "Prediction 2: what: 26.68%\n",
      "Prediction 3: tomorrow: 12.51%\n",
      "Prediction 4: you: 3.90%\n",
      "Prediction 5: go: 2.96%\n",
      "Bone Motion Model:\n",
      "Prediction 1: name: 80.93%\n",
      "Prediction 2: school: 13.22%\n",
      "Prediction 3: yesterday: 5.41%\n",
      "Prediction 4: self: 0.27%\n",
      "Prediction 5: go: 0.08%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "\n",
    "# remove unnecessary module. from state_dict\n",
    "joint_state_dict = OrderedDict()\n",
    "bone_state_dict = OrderedDict()\n",
    "joint_motion_state_dict = OrderedDict()\n",
    "bone_motion_state_dict = OrderedDict()\n",
    "for i, model in enumerate([joint_model, bone_model, joint_motion_model, bone_motion_model]):\n",
    "    for k, v in model.items():\n",
    "        #name = k[7:] # remove 'module.'\n",
    "        name = k.replace('module.', '')\n",
    "        if i == 0:\n",
    "            joint_state_dict[name]=v\n",
    "        elif i == 1:\n",
    "            bone_state_dict[name]=v\n",
    "        elif i == 2:\n",
    "            joint_motion_state_dict[name]=v\n",
    "        elif i == 3:\n",
    "            bone_motion_state_dict[name]=v\n",
    "\n",
    "# Load model architechure\n",
    "Model_j = decouple_gcn_attn.Model(13, 27, 1, 16, 41,  \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_b = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_jm = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "Model_bm = decouple_gcn_attn.Model(13, 27, 1, 16, 41, \"sign_27.Graph\", {\"labeling_mode\": 'spatial'}, 3)\n",
    "\n",
    "# Load model states from checkpoints\n",
    "Model_j.load_state_dict(joint_state_dict)\n",
    "Model_b.load_state_dict(bone_state_dict)\n",
    "Model_jm.load_state_dict(joint_motion_state_dict)\n",
    "Model_bm.load_state_dict(bone_motion_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "Model_j.eval()\n",
    "Model_b.eval()\n",
    "Model_jm.eval()\n",
    "Model_bm.eval()\n",
    "\n",
    "gcn_predictions = []\n",
    "\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/sign_gen/'):\n",
    "        bone_npy = np.load(path_to_video + folder + '/sign_gen/test_data_bone.npy')\n",
    "        joint_npy = np.load(path_to_video + folder + '/sign_gen/test_data_joint.npy')\n",
    "        bone_motion_npy = np.load(path_to_video + folder + '/sign_gen/test_data_bone_motion.npy')\n",
    "        joint_motion_npy = np.load(path_to_video + folder + '/sign_gen/test_data_joint_motion.npy')\n",
    "\n",
    "        # Load the data onto the GPU if available\n",
    "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        bone_npy = torch.from_numpy(bone_npy).to(device)\n",
    "        joint_npy = torch.from_numpy(joint_npy).to(device)\n",
    "        bone_motion_npy = torch.from_numpy(bone_motion_npy).to(device)\n",
    "        joint_motion_npy = torch.from_numpy(joint_motion_npy).to(device)\n",
    "\n",
    "        # Make predictions using the four models\n",
    "        with torch.no_grad():\n",
    "            joint_output = Model_j(joint_npy)\n",
    "            bone_output = Model_b(bone_npy)\n",
    "            joint_motion_output = Model_jm(joint_motion_npy)\n",
    "            bone_motion_output = Model_bm(bone_motion_npy)\n",
    "\n",
    "        # Print the top 5 predictions and their confidence percentages for each model\n",
    "        def print_top_5(output):\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "            for i in range(5):\n",
    "                print(f\"Prediction {i+1}: {class_labels[top_5_indices[0][i]]}: {top_5_probs[0][i]*100:.2f}%\")\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        print(\"Joint Model:\")\n",
    "        print_top_5(joint_output)\n",
    "        print(\"Bone Model:\")\n",
    "        print_top_5(bone_output)\n",
    "        print(\"Joint Motion Model:\")\n",
    "        print_top_5(joint_motion_output)\n",
    "        print(\"Bone Motion Model:\")\n",
    "        print_top_5(bone_motion_output)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Predictions (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for set_0:\n",
      "Prediction 1: self: 73.46%\n",
      "Prediction 2: you: 18.64%\n",
      "Prediction 3: tomorrow: 4.66%\n",
      "Prediction 4: we: 0.74%\n",
      "Prediction 5: hello: 0.56%\n",
      "\n",
      "\n",
      "Predictions for set_1:\n",
      "Prediction 1: tomorrow: 31.99%\n",
      "Prediction 2: car: 15.06%\n",
      "Prediction 3: you: 11.89%\n",
      "Prediction 4: have: 11.83%\n",
      "Prediction 5: go: 7.91%\n",
      "\n",
      "\n",
      "Predictions for set_2:\n",
      "Prediction 1: have: 50.89%\n",
      "Prediction 2: school: 9.70%\n",
      "Prediction 3: tomorrow: 7.17%\n",
      "Prediction 4: go: 6.96%\n",
      "Prediction 5: self: 6.69%\n",
      "\n",
      "\n",
      "Predictions for set_3:\n",
      "Prediction 1: name: 33.27%\n",
      "Prediction 2: school: 17.47%\n",
      "Prediction 3: tomorrow: 13.11%\n",
      "Prediction 4: you: 7.20%\n",
      "Prediction 5: hello: 6.41%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_labels = [\"car\", \"go\", \"have\", \"hello\", \"my\", \"name\", \"school\", \"self\", \"tomorrow\", \"we\", \"what\", \"yesterday\", \"you\"]\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = T_Pose_model(frames_number=60,joints_number=33,\n",
    "    n_classes=13\n",
    ")\n",
    "#model = nn.DataParallel(model)    \n",
    "model = model.to(device)\n",
    "tcn_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in tcn_model.items():\n",
    "    #name = k[7:] # remove 'module.'\n",
    "    name = k.replace('module.', '')\n",
    "    tcn_state_dict[name]=v\n",
    "\n",
    "# Add weights from checkpoint model\n",
    "model.load_state_dict(tcn_state_dict)#,strict=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for folder in os.listdir(path_to_video):\n",
    "    if os.path.isdir(path_to_video + folder + '/pt/'):\n",
    "        pt_file = path_to_video + folder + '/pt/0.mp4.pt'\n",
    "        data = torch.load(pt_file)\n",
    "        #data = data.contiguous().view(1,-1,24,24)\n",
    "        data_in = torch.autograd.Variable(data.to(device), requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            pred=model(data_in)\n",
    "        #pred = pred.cpu().detach().numpy()\n",
    "        def print_top_5(output):\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "            for i in range(5):\n",
    "                print(f\"Prediction {i+1}: {class_labels[top_5_indices[0][i]]}: {top_5_probs[0][i]*100:.2f}%\")\n",
    "        print(\"Predictions for \" + folder + \":\")\n",
    "        print_top_5(pred)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f37eb7a0b74f2e22d6fd383ec5ebb97eab96881d30ca476017b85fc7b23292a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
